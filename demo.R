##### 
##### 
# This is a demo using two subsets of ED data cleaned from MIMIC-IV (https://physionet.org/content/mimiciv/0.4/)
# See data pre-processing details at https://github.com/nliulab/mimic4ed-benchmark
# The outcome used in this demo is hospitalization (binary).
##### 
library(AutoScore)
library(tidyverse)
library(ggplot2)
library(mle.tools)
library(rjson)

source("R/fedscore.R")
source("R/helpers.R")

##### Import data:
dat1 <- read.csv("SampleData/sample1.csv")
dat2 <- read.csv("SampleData/sample2.csv")
nrow(dat1) # sample size 2000
nrow(dat2) # sample size 3000
### format data:
var = colnames(dat1)[-1]
dat1 = dat1[, var]
dat2 = dat2[, var]
factor_names <- c("gender", "cci_CHF", "cci_Stroke", "cci_Renal")
dat1[factor_names] <- lapply(dat1[factor_names] , factor)
dat2[factor_names] <- lapply(dat2[factor_names] , factor)
### Partition data at each site into training, validation and testing sets:
spec = c(train = .7, test = .2, validate = .1)
# site1:
g = sample(cut(
  seq(nrow(dat1)), 
  nrow(dat1)*cumsum(c(0,spec)),
  labels = names(spec)
))
dat1_split = split(dat1, g)
# site2:
g = sample(cut(
  seq(nrow(dat2)), 
  nrow(dat2)*cumsum(c(0,spec)),
  labels = names(spec)
))
dat2_split = split(dat2, g)


##### Module1. Obtain federated ranking
# each site first obtain and write ranks independently:
rank.s1 <- AutoScore::AutoScore_rank(dat1_split$train)
write_rank(rank.s1, "outputs/ranks/rank1.csv")
rank.s2 <- AutoScore::AutoScore_rank(dat2_split$train)
write_rank(rank.s2, "outputs/ranks/rank2.csv")
# then get global rank:
rank.fed <- AutoScore_Frank(K=2) # by using results generated by RF at each site

# take a look at the rank:
rank.fed[,c("var", "rank1", "rank2", "rank_fed")]
# the rank in this demo is highly homogeneous across different sites

# get the variable list in the order of federated ranking:
var.rank = rank.fed$var[1:9]


##### 
##### 
##### Module2
# each site independently write cut off into cutoff.json
write_cutoff(df = dat1_split$train, sitename = "homo_S1", path = "cutoff.json")
write_cutoff(df = dat2_split$train, sitename = "homo_S2", path = "cutoff.json")
# decide uniform cutoff:
uni_cut <- get_uni_cut(c("homo_S1", "homo_S2"), path = "cutoff.json")
uni_cut


##### 
##### 
##### Module 3: Conduct federated logistic regression
##### 

##### Use site 1 as local site
##### re-set directory:
names_all_site = c("homo_S1", "homo_S2")
cutoff = uni_cut
# 1. write site_beta.json independently at each site:
  write_Lbeta("homo_S1", names_all_site, dat1_split$train, var.rank, cutoff) # site1
# 2. write site_intermediate.json using data at site 2:
  write_Ds("homo_S1", "homo_S2", names_all_site, dat2_split$train, var.rank, cutoff)
# 3. do federated LR
dat1_train_transformed <- transform_df_fixed(dat1_split$train, uni_cut)
  # loop though all variables:
  for(i in 1:9){
    print(i)
    var_m = var.rank[1:i]
    model <- get_federatedLR(method = 1, data_matrix = dat1_train_transformed[,c("label", var_m)], sitenameL = "homo_S1", sitenameR_list = "homo_S2", modelIndex = i)
    coef = model$beta_global %>% as.data.frame()
    csv_name = sprintf("outputs/coef_global_homoS1_train_model%d.csv", i)
    write.csv(coef, csv_name)
}



##### 
##### 
##### Module 4 & 5: read the coef estimated by federated logistic regression and get scoring tables, and testing results
##### 

# reorder cut list
# # the order of cut list must be consistent to the order of variable ranking
#  should always double check
uni_cut_fed = list(uni_cut$age, uni_cut$triage_MAP, uni_cut$triage_heartrate, uni_cut$triage_temperature, uni_cut$triage_o2sat)
names(uni_cut_fed) = c("Age", "MAP", "O2sat", "heartrate", "temperature")
# 
# get validation results:
sink("outputs/homoS1_auc_validation_fed.txt")
# loop through all 9 variables
for(i in 1:9){
  var = var.rank[1:i]
  csv_name = sprintf("outputs/coef_global_homoS1_train_model%d.csv",i)
  coef = read.csv(csv_name)
  coef_vec = transform_score(coef$.)
  names(coef_vec) = coef$X
  df = dat1_train_transformed[, var] %>% as.data.frame()
  colnames(df) = var
  score_tbl = add_baseline(df, transform_scoretbl(coef_vec))
  # use each validation sets:
    print(sprintf("generating auc for Site1 validation for model %d", i))
    pred <- AutoScore::AutoScore_testing(dat1_split$validate, var, uni_cut_fed, score_tbl, threshold = "best", with_label = TRUE)
    print(sprintf("generating auc for Site2 validation for model %d", i))
    pred <- AutoScore::AutoScore_testing(dat2_split$validate, var, uni_cut_fed, score_tbl, threshold = "best", with_label = TRUE)
}
sink()

# Use extract_fed.py to extract results from text to csv
# Sample command: Python3 Python/extract_fed.py outputs/homoS1_auc_validation_fed.txt outputs/fed.csv

### plot parsimony plot from fed.csv
### 
### 
### 
### 
### site1
df = read.csv("outputs/fed.csv")
# We offer following two options for visualizations. 
# Users may set up other ways; see more discussion in the manuscript
# plot 1
df %>% group_by(Model) %>% 
  summarize(AUC_mean = mean(AUC)) %>% 
  ggplot(aes(x=Model, y = AUC_mean)) +  
  geom_line() + 
  geom_point()
# plot 2
df_new = df %>% group_by(Model) %>% summarize(AUC_mean = mean(AUC))
df_new = cbind(df_new, var.rank[1:9])
colnames(df_new) = c("Model", "AUC_mean", "Var")
# use bar plots to stay consistent with AutoScore
var_names <- factor((var.rank)[1:9], levels = (var.rank)[1:9])
dt <- data.frame(AUC_mean = df_new$AUC_mean, variables = var_names, num = 1:9)
auc_lim_max <- ceiling(max(df$AUC)*10)/10
p = ggplot(data = dt, mapping = aes_string(x = "variables", y = "AUC_mean")) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_cartesian(ylim = c(0, auc_lim_max))+
  theme_bw() +
  labs(x = "", y = "Area Under the Curve", title = "Parsimony plot on the validation set") +
  theme(legend.position = "none",
        text = element_text(size = 20),
        axis.text = element_text(size = 18),
        axis.text.y = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        plot.title = element_text(size=25, hjust = 0.5))
print( p + geom_text(aes(label = dt$num), vjust = 1.5, colour = "white"))
# Suppose we decide to select the first 6 variables for final model
# This is a subjective decision made by users, as discussed in the manuscript.

# Now fit new (final) logistic regression based on results of parsimony plot:
sink("outputs/fed_AUC_S1_with_scoretbl_var6.txt")

var_fed = var.rank[1:6]
coef = read.csv("outputs/coef_global_homoS1_train_model6.csv")
coef_vec = transform_score(coef$.)
names(coef_vec) = coef$X
df = dat1_train_transformed[, var_fed] %>% as.data.frame()
colnames(df) = var_fed
score_tbl_fed = add_baseline(df, transform_scoretbl(coef_vec))
print("printing score tbl")
score_tbl_fed %>% print
# test on testing sets independently:
print("generating auc, testing set = site1")
pred_SGH1 <- AutoScore::AutoScore_testing(dat1_split$test, var_fed, uni_cut_fed, score_tbl_fed, threshold = "best", with_label = TRUE)
print("generating auc, testing set = site2")
pred_SGH1 <- AutoScore::AutoScore_testing(dat2_split$test, var_fed, uni_cut_fed, score_tbl_fed, threshold = "best", with_label = TRUE)
sink()







