########################################
########################################
########################################
# Change the existed AutoScore framework to federated
source("functions_federatedLR.R")
source("Federated AutoScore_internalfunc.R")
source("./AutoScore-master/R/AutoScore.R")

############################################################
############################################################
library(tidyverse)

# ranking by XGBoost
# return vector of "Gain" with names
# rank_XGBoost <- function(data_train){
#   var = colnames(data_train)
#   var = var[!(var =="label")]
#   length(var) %>% print
#   dtrain <- xgb.DMatrix(data = data.matrix(data_train[, var]), label= as.numeric(data_train$label))
#   # train a model using our training data
#   model_xgboost <- xgboost(data = dtrain,   
#                            nround = 500,
#                            max_depth = 10,
#                            eta = 0.3, 
#                            nthread = 2,
#                            booster = "gblinear",
#                            objective = "binary:logistic")  
#   # importance:
#   importance_XG <- xgb.importance(feature_names = var, model = model_xgboost)
#   result = importance_XG$Weight
#   names(result) = importance_XG$Feature
#   result = sort(result, decreasing = T)
#   return(result)
# }


# method1: separate ranking at each site, combine and get uniformed ranking
# method2: use shapleyVIC, SAGE calculated using local site data
# if choose to use method1, input ranking r1, r2, r3, which is generated by AutoScore_rank() at each site
# if choose to use method2, 
# must ensure ranking_site_beta.json and ranking_intermediate.json are update already.
AutoScore_Frank <- function(method = 1, weight = NULL, K = 2){
  if(method==1){
    getwd() %>% print
    r1 = read.csv("rank1.csv")
    r2 = read.csv("rank2.csv")
    # r3 = read.csv("rank3.csv")
    colnames(r1) = c("var", "imp_1", "rank1")
    colnames(r2) = c("var", "imp_2", "rank2")
    # colnames(r3) = c("var", "imp_3", "rank3")

    tbl <- left_join(r1, r2)

    tbl$rank_avg <- (tbl$rank1 + tbl$rank2)/K
    tbl <- tbl %>% arrange(rank_avg)
    tbl$rank_fed = c(1:nrow(tbl))
    return(tbl)
  }
  if(method ==2){
    # to be finished later
    # add a step to verify json files are updated already
    return(NULL)
  }
}


############################################################
############################################################
# Module 2: variable transformation
# Module 3: Score Derivation by Weighting and Normalization
# 
# federated version of AutoScore_parsimony
# currently don't consider cross validation
# input: 
#       validation_set
#       variableList
#       cutoff: uniformed cutoff
#       coef: vec of coeffient of global model
# currently don't consider cross validation       
# AutoScore_Fparsimony <- function(validiation_set, max_score=100, cutoff, variableList, score){
#   
#   # transform validation_set cts var to categorical:
#   df = transform_df_fixed(validiation_set, cutoff)
#   # assign score:
#   dat = assign_score(df, score)
#   # loop through all models to get vector of AUC
#   K = length(variableList)
#   AUC <- c()
#   for(i in 1: K){
#     print(i)
#   # calculate pred:
#     variableList[1:i] %>% print
#     colnames(dat) %>% print
#     subset(dat, select = variableList[1:i])
#     pred = rowSums(subset(dat, select = variableList[1:i]))
#     obj <- roc(response = as.vector(dat$label), predictor = as.vector(pred))
#     AUC <- c(AUC, auc(obj))
#   }
#   
#   print(AUC)
#   
#   # produce parsimony plot:
#   plot(c(1:K), AUC)
#   lines(c(1:K), AUC, type="b", col="green", lwd=2, pch=19)
#   return(AUC)
# }
# read from csv file with updated auc values
# AutoScore_Fparsimony <- function(filename, var_list){
#   df = read.csv(filname)
#   if(nrow(df)!=length(var_list)){
#     print("error! file and var list don't match")
#     return()
#   }
#   
#   # psrsimony plot:
#   ggplot(df, aes(x = Model, y = AUC, colour =SGH, group = SGH)) +  geom_line() + geom_point()
# }



# input: data_train at local site
#        site_list, vector containing site names consistent with cutoff.json   
#        variableList, vector containing all variables, same order to cutoff.json  
# output: write score table to json file
# AutoScore_Fweighting <- function(data_train, site_list, site_nameL, max_score, variableList){
# 
# # Module 2: variable transformation
#   cut_uni <- get_uni_cut(get_all_cut(variableList, site_list))
# # Module 3: Score Derivation by Weighting and Normalization  
#   score_table <- score_Fweighting(data_train, max_score, site_list, site_nameL,variableList)
#   cat("****Initial Scores: \n")
#   print_scoring_table(scoring_table = score_table, final_variable = variableList)
#   
#   return(score_table)
#   
# }

# intermediate evaluation by roc analysis
# input: validation set at any site
#        sitename
# output: write cut_vec into json file
# intermediate_roc <- function(score_table, data_validation, sitename){
#   # read score table from json file
#   # score_table <- 
#   # Using "assign_score" to generate score based on new dataset and Scoring table "score_table"
#   validation_set <- assign_score(data_validation, score_table)
#   validation_set_3$total_score <-
#     rowSums(subset(validation_set_3, select = names(validation_set_3)[names(validation_set_3) !=
#                                                                         "label"]))
#   y_validation <- validation_set_3$label
#   
#   # Intermediate evaluation based on Validation Set
#   plot_roc_curve(validation_set_3$total_score, as.numeric(y_validation) - 1)
#   print(sprintf("Performance (based on validation set at site %s)"), sitename)
#   print_roc_performance(data_validation$y, validation_set_3$total_score, threshold = "best")
#   print(sprintf("The cutoffs of each variable generated by the AutoScore are saved in cut_vec. You can decide whether to revise or fine-tune them"))
#   return(cut_vec)
# }
# 
# # read from json files to ?
# intermediate_roc_all <- function(score_table){
#   
# }

############################################################
############################################################
# Module 4: Model Selection and Parameter Determination

############################################################
############################################################
# Module 5: Fine-Tuning Cutoff Points in the Variable Transformation

############################################################
############################################################
# Module 6: Predictive Performance Evaluation





